{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KDT_AI-classifying prejudice and discrimination texts\n",
    "https://www.kaggle.com/competitions/kdtai-2/overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "from gensim.models import word2vec\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from python_scripts import data_setup, engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_class = {\n",
    "  0: 'Origin(출신차별)',\n",
    "  1: 'Physical(외모차별) 외모(신체, 얼굴) 및 장애인 차별 발언을 포함합니다.',\n",
    "  2: 'Politics(정치성향차별)',\n",
    "  3: 'Profanity(혐오욕설) 욕설,저주,혐오 단어, 비속어 및 기타 혐오 발언을 포함합니다.',\n",
    "  4: 'Age(연령차별)',\n",
    "  5: 'Gender(성차별) 성별 또는 성적 취향에 대한 차별 발언을 포함합니다.',\n",
    "  6: 'Not Hate Speech(해당사항없음)',\n",
    "}\n",
    "\n",
    "max_length = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>유소영비호감 성형아줌마</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>나오지마라 썅</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>식상하고 지긋지긋했는데 잘 끝나네 오예 소리벗고 빤스질러~~!!!</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>성희롱 당할 얼굴이 아닌데?ㅋㅋㅋ</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>\"끝까지 해보자~쪽파리 원숭이 자한 쓰레기당\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65858</th>\n",
       "      <td>65858</td>\n",
       "      <td>ㅋ ㅋ 쇼~~~ 도 적당히</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65859</th>\n",
       "      <td>65859</td>\n",
       "      <td>\"이젠 전라도 종것들 음식 불매다. 태양광 중금속 환경오염물로 만든 식품 사절이다\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65860</th>\n",
       "      <td>65860</td>\n",
       "      <td>조센징들은 참 피곤하게 산다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65861</th>\n",
       "      <td>65861</td>\n",
       "      <td>\"문빠 다모아서 빨갱이한테보내고 행복하게살라고\"</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65862</th>\n",
       "      <td>65862</td>\n",
       "      <td>항마력 딸려서 못보겟음 보는사람들 대부분 환상쩌는여자일듯..</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65863 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                                            text  label\n",
       "0          0                                    유소영비호감 성형아줌마      1\n",
       "1          1                                         나오지마라 썅      3\n",
       "2          2            식상하고 지긋지긋했는데 잘 끝나네 오예 소리벗고 빤스질러~~!!!      6\n",
       "3          3                              성희롱 당할 얼굴이 아닌데?ㅋㅋㅋ      5\n",
       "4          4                       \"끝까지 해보자~쪽파리 원숭이 자한 쓰레기당\"      0\n",
       "...      ...                                             ...    ...\n",
       "65858  65858                                  ㅋ ㅋ 쇼~~~ 도 적당히      6\n",
       "65859  65859  \"이젠 전라도 종것들 음식 불매다. 태양광 중금속 환경오염물로 만든 식품 사절이다\"      0\n",
       "65860  65860                                 조센징들은 참 피곤하게 산다      0\n",
       "65861  65861                      \"문빠 다모아서 빨갱이한테보내고 행복하게살라고\"      2\n",
       "65862  65862               항마력 딸려서 못보겟음 보는사람들 대부분 환상쩌는여자일듯..      5\n",
       "\n",
       "[65863 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('../data/Discrimination/train.csv')\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "306"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(s) for s in train_data['text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_korean_text(text):\n",
    "    # Remove URLs and mentions <= 본 문제에서는 url 이나 email 주소가 나오지 않으니 그리 크게 중요하지 않을듯\n",
    "    text = re.sub(r\"(http|https)?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\", \"\", text)\n",
    "    text = re.sub(r\"@(\\w+)\", \"\", text)\n",
    "\n",
    "    # Tokenize text using Mecab <= 이 부분은 다른 라이브러리로 바꾸어도 무방함\n",
    "    mecab = Mecab()\n",
    "    tokens = mecab.morphs(text)\n",
    "\n",
    "    # Remove stop words (optional) <= 필요하다면 바꾸어 보아도 됨\n",
    "    stop_words = [\"은\", \"는\", \"이\", \"가\", \"을\", \"를\", \"에\", \"의\", \"로\", \"으로\", \"에서\"]\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "\n",
    "    # Remove punctuation and non-Korean characters <= 필요하다면 바꾸어 보아도 됨\n",
    "    tokens = [re.sub(r\"[^\\u3131-\\u3163\\uac00-\\ud7a3]+\", \"\", t) for t in tokens]\n",
    "    tokens = [t for t in tokens if t]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['나', '지금', '뭐', '하', '고', '있', '느냐']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_korean_text('나는 지금 뭐하고 있느냐?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Getting Word2Vec embedding pre-trained model <= 1번 word-embedding model\n",
    "\n",
    "# w2v_pretrained_model = word2vec.Word2Vec.load('../data/Discrimination/word2vec')\n",
    "# w2v_pretrained_model.wv.add_vector('<unk>', [0.0] * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Getting GloVe embedding pre-trained model <= 2번 word-embedding model\n",
    "\n",
    "# def load_glove_model(file):\n",
    "#     print(\"Loading Glove Weight\")\n",
    "#     glove_vector = {}\n",
    "#     with open(file,'r') as f:\n",
    "#         for line in f:\n",
    "#             split_line = line.split()\n",
    "#             word = split_line[0]\n",
    "#             embedding = np.array(split_line[1:], dtype=np.float64)\n",
    "#             glove_vector[word] = embedding\n",
    "\n",
    "#     # 없는 단어 추가 <= 아래 ref 1. 성능 변화 미미함\n",
    "#     not_exists = pd.read_csv('../data/Discrimination/not_exist_list.csv')\n",
    "#     for i in range(len(not_exists)):\n",
    "#         glove_vector[not_exists.loc[i, 'word']] = glove_vector[not_exists.loc[i, 'substitute']]\n",
    "\n",
    "#     class Word_vector():\n",
    "#         def __init__(self, key_to_vector) -> None:\n",
    "#             self.key_to_vector = key_to_vector\n",
    "\n",
    "#             self.index_to_key = []\n",
    "#             self.key_to_index = {}\n",
    "#             for key in self.key_to_vector.keys():\n",
    "#                 self.index_to_key.append(key)\n",
    "#                 self.key_to_index[key] = len(self.index_to_key) - 1\n",
    "\n",
    "#             self.vectors = []\n",
    "#             for i in range(len(self.index_to_key)):\n",
    "#                 self.vectors.append(self.key_to_vector[self.index_to_key[i]])\n",
    "#             self.vectors = np.array(self.vectors, dtype='float32')\n",
    "\n",
    "#             self.vector_size = len(self.vectors[0])\n",
    "\n",
    "#         def __contains__(self, key):\n",
    "#             return key in self.key_to_vector\n",
    "\n",
    "#         def __getitem__(self, key):\n",
    "#             return self.key_to_vector[key]\n",
    "\n",
    "#         def __len__(self):\n",
    "#             return len(self.index_to_key)\n",
    "\n",
    "#     class Glove_model():\n",
    "#         def __init__(self, vector) -> None:\n",
    "#             self.wv = Word_vector(vector)\n",
    "\n",
    "#         def __len__(self):\n",
    "#             return len(self.wv)\n",
    "\n",
    "#     glove_model = Glove_model(glove_vector)\n",
    "\n",
    "#     print(f\"{len(glove_model)} words loaded!\")\n",
    "#     return glove_model\n",
    "\n",
    "# glove_pretrained_model = load_glove_model('../data/Discrimination/glove.txt')\n",
    "# len(glove_pretrained_model.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref 1. pre-trained Glove 임베딩 벡터에 등록되지 않은 단어 중 주요 단어를 골라 수작업 처리 <= 성능 영향 미미함\n",
    "# all = 0\n",
    "# not_exists = {}\n",
    "# not_exists_labels = {}\n",
    "\n",
    "# for i in tqdm_notebook(range(len(train_data))):\n",
    "#     sentence, label = train_data.loc[i, 'text'], train_data.loc[i, 'label']\n",
    "#     for word in preprocess_korean_text(sentence):\n",
    "#         if word not in glove_pretrained_model.wv:\n",
    "#             if word in not_exists:\n",
    "#                 not_exists[word] += 1\n",
    "#             else:\n",
    "#                 not_exists[word] = 1\n",
    "\n",
    "#             if word not in not_exists_labels:\n",
    "#                 not_exists_labels[word] = {n: 0 for n in range(7)}\n",
    "#             not_exists_labels[word][label] += 1\n",
    "#         all += 1\n",
    "\n",
    "# not_exists = sorted(not_exists.items(), key=lambda x: x[1], reverse=True)\n",
    "# not_labels = []\n",
    "# for word, n in not_exists:\n",
    "#     not_labels.append(sorted(not_exists_labels[word].items(), key=lambda x: x[1], reverse=True))\n",
    "# print(all, len(not_exists))\n",
    "# print(not_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not_exist_list = pd.DataFrame({'word': not_exists, 'label': not_labels})\n",
    "# not_exist_list.to_csv('../data/Discrimination/not_exist_list.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d7d2b4dc29945b5b21e1ee9e42015d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Making word maps:   0%|          | 0/65863 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "word_index_to_key = []\n",
    "word_key_to_index = {}\n",
    "\n",
    "for i in tqdm_notebook(range(len(train_data)), 'Making word maps'):\n",
    "    text = train_data.iloc[i]['text']\n",
    "    tokens = preprocess_korean_text(text)\n",
    "\n",
    "    for token in tokens:\n",
    "        if token not in word_key_to_index:\n",
    "            word_key_to_index[token] = len(word_index_to_key)\n",
    "            word_index_to_key.append(token)\n",
    "\n",
    "word_key_to_index['<unk>'] = len(word_index_to_key)\n",
    "word_index_to_key.append('<unk>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KoreanTextDataset(Dataset):\n",
    "    def __init__(self, data, preprocess_korean_text, max_length=100):\n",
    "        self.data = data\n",
    "        self.max_length = max_length\n",
    "        self.preprocess_korean_text = preprocess_korean_text\n",
    "        self.idx_to_class = sorted(data['label'].unique())\n",
    "        self.class_to_idx = {}\n",
    "        for i in range(len(self.idx_to_class)):\n",
    "            self.class_to_idx[self.idx_to_class[i]] = i\n",
    "        self.class_names = self.idx_to_class\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.data.loc[index, \"text\"]\n",
    "        label = self.data.loc[index, \"label\"]\n",
    "\n",
    "        # Preprocess text using the preprocess_korean_text() function\n",
    "        tokens = self.preprocess_korean_text(text)\n",
    "        # Truncate or pad tokens to a fixed length\n",
    "        if len(tokens) > self.max_length:\n",
    "            tokens = tokens[:self.max_length]\n",
    "        else:\n",
    "            tokens += [\"\"] * (self.max_length - len(tokens))\n",
    "\n",
    "        # Convert tokens to indices using the pre-trained GloVe or Word2Vec embeddings\n",
    "        indices = []\n",
    "        for token in tokens:\n",
    "            if token in word_key_to_index:\n",
    "                indices.append(word_key_to_index[token])\n",
    "            else:\n",
    "                indices.append(word_key_to_index['<unk>'])  # use the index of the <unk> token for out-of-vocabulary words\n",
    "\n",
    "        return torch.tensor(indices), torch.tensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Splitting dataset of length 65863 into splits of size: 59276 and 6587\n"
     ]
    }
   ],
   "source": [
    "train_dataset = KoreanTextDataset(\n",
    "    data=train_data,\n",
    "    preprocess_korean_text=preprocess_korean_text,\n",
    "    max_length=max_length\n",
    ")\n",
    "\n",
    "train_dataset_sub, val_dataset_sub = data_setup.split_dataset(\n",
    "    dataset=train_dataset,\n",
    "    split_size=0.9,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "        self.v = nn.Linear(hidden_size, 1, bias=False)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        max_len = encoder_outputs.size(0)\n",
    "\n",
    "        repeated_hidden = hidden.unsqueeze(0).repeat(max_len, 1, 1)\n",
    "\n",
    "        energy = torch.tanh(self.attn(torch.cat((repeated_hidden, encoder_outputs), dim=2)))\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        return F.softmax(attention, dim=0).unsqueeze(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_LSTM_attention(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, LSTM_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        for param in self.embedding.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=LSTM_layers, bidirectional=False, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attention = Attention(hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, text):\n",
    "        # text = [batch size, seq len]\n",
    "\n",
    "        embedded = self.embedding(text)\n",
    "        embedded = self.dropout(embedded)\n",
    "        # embedded = [batch size, seq len, emb dim]: [64, 200, 100]\n",
    "        # print('embedded: ', embedded.shape)\n",
    "\n",
    "        lstm_outputs, (hidden, _) = self.lstm(embedded.permute(1, 0, 2))\n",
    "        # output = [batch size, seq len, hid dim * num directions]: [200, 64, 512]\n",
    "        # hidden/cell = [num layers * num directions, batch size, hid dim]: [6, 64, 512]\n",
    "        # print('outputs, hidden: ', lstm_outputs.shape, hidden.shape)\n",
    "\n",
    "        attention_weights = self.attention(hidden[-1, :, :], lstm_outputs)\n",
    "        # # attention_weights = [batch size, seq len, 1]: [200, 64, 1]\n",
    "        # print('attention_weights: ', attention_weights.shape)\n",
    "\n",
    "        context_vector = torch.bmm(lstm_outputs.permute(1, 2, 0), attention_weights.permute(1, 0, 2)).squeeze(2)\n",
    "        # # context_vector = [batch size, hid dim * num directions]: [64, 512]\n",
    "        # print('context_vector: ', context_vector.shape)\n",
    "\n",
    "        out = self.fc(self.dropout(context_vector.squeeze(0)))\n",
    "        # out = [batch size, output dim]: [64, 7]\n",
    "        # print('out: ', out.shape)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_list = [1e-3, 1e-4] # 각 LR 별로 10 epoch 씩 연달아 학습 진행\n",
    "weight_decay_list = [1e-4]\n",
    "epochs_list = [8]\n",
    "batch_size_list = [64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 1, 2, 3, 4, 5, 6], 7)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names, num_classes = train_dataset.class_names, len(train_dataset.class_names)\n",
    "class_names, num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN_LSTM_attention(\n",
    "    vocab_size=len(word_index_to_key),\n",
    "    embedding_dim=100,\n",
    "    hidden_dim=512,\n",
    "    output_dim=num_classes,\n",
    "    LSTM_layers=2,\n",
    "    dropout=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "RNN_LSTM_attention                       --\n",
       "├─Embedding: 1-1                         (3,674,200)\n",
       "├─LSTM: 1-2                              3,358,720\n",
       "├─Dropout: 1-3                           --\n",
       "├─Attention: 1-4                         --\n",
       "│    └─Linear: 2-1                       524,800\n",
       "│    └─Linear: 2-2                       512\n",
       "├─Linear: 1-5                            3,591\n",
       "=================================================================\n",
       "Total params: 7,561,823\n",
       "Trainable params: 3,887,623\n",
       "Non-trainable params: 3,674,200\n",
       "================================================================="
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ffd96754b19441ab8118ee978b78ee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LSTM_attention_discrimination_LR_0.001_WD_0.0001_BS_64_GA_1:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f463f6103f31486f85cef3175a6bf682",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/927 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7de57491fce42938c019695ff3c4942",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Train_loss: 1.1555, Train_acc: 0.6091 | Test_loss: 0.9108, Test_acc: 0.7016\n",
      "[INFO] Saving model to: ../models/LSTM_attention_discrimination_LR_0.001_WD_0.0001_BS_64_GA_1_EPOCH_0_TEST-ACC_0.7016.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe336224dab9477db8437cc3e470d598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/927 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5052dc5b7ba04f11998205a70b4f0dbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Train_loss: 0.8888, Train_acc: 0.7009 | Test_loss: 0.8268, Test_acc: 0.7197\n",
      "[INFO] Saving model to: ../models/LSTM_attention_discrimination_LR_0.001_WD_0.0001_BS_64_GA_1_EPOCH_1_TEST-ACC_0.7197.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06134e03b0ec432f90b07c78b370597f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/927 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc88487462604b0db4a9de9f15b69157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Train_loss: 0.8169, Train_acc: 0.7253 | Test_loss: 0.7821, Test_acc: 0.7356\n",
      "[INFO] Saving model to: ../models/LSTM_attention_discrimination_LR_0.001_WD_0.0001_BS_64_GA_1_EPOCH_2_TEST-ACC_0.7356.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ccdb5a417c74b11a931a1a280530fb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/927 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tuning_results = engine.HP_tune_train(\n",
    "    model=model,\n",
    "    model_generator=None,\n",
    "    model_weights=None,\n",
    "    model_name='LSTM_attention_discrimination',\n",
    "    train_dataset=train_dataset_sub,\n",
    "    test_dataset=val_dataset_sub,\n",
    "    class_names=class_names,\n",
    "    learning_rate_list=learning_rate_list,\n",
    "    weight_decay_list=weight_decay_list,\n",
    "    epochs_list=epochs_list,\n",
    "    batch_size_list=batch_size_list,\n",
    "    is_tensorboard_writer=False,\n",
    "    device=device,\n",
    "    gradient_accumulation_num=1,\n",
    "    saving_max=True,\n",
    "    metric_learning=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_weight = torch.load('..\\models\\discrimination\\LSTM_attention_glove_discrimination_LR_0.0001_WD_0.0001_BS_64_GA_1_EPOCH_0_TEST-ACC_0.7751.pth')\n",
    "model.load_state_dict(loaded_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('../data/Discrimination/test.csv')\n",
    "labels = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.inference_mode():\n",
    "  for i in tqdm_notebook(range(len(test_data))):\n",
    "    test_text = test_data.loc[i, \"text\"]\n",
    "    test_tokens = preprocess_korean_text(test_text)\n",
    "    if len(test_tokens) > max_length:\n",
    "        test_tokens = test_tokens[:max_length]\n",
    "    else:\n",
    "        test_tokens += [\"\"] * (max_length - len(test_tokens))\n",
    "\n",
    "    indices = []\n",
    "    for token in test_tokens:\n",
    "      if token in word_key_to_index:\n",
    "        indices.append(word_key_to_index[token])\n",
    "      else:\n",
    "        indices.append(word_key_to_index['<unk>'])  # use the index of the <unk> token for out-of-vocabulary words\n",
    "\n",
    "    test_logits = model(torch.tensor(indices).unsqueeze(0).to(device))\n",
    "    labels.append(class_names[torch.argmax(test_logits.squeeze(0).cpu())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['label'] = [idx_to_class[label] for label in labels]\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_data = pd.DataFrame({'ID': range(len(test_data)), 'label': labels})\n",
    "submission_data.to_csv('../submissions/discrimination/submission.csv', index=False)\n",
    "print('submission completed!')\n",
    "submission_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
