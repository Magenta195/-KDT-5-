{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KDT_AI-classifying prejudice and discrimination texts\n",
    "https://www.kaggle.com/competitions/kdtai-2/overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tglim/miniforge3/envs/tensorflow/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: dlopen(/Users/tglim/miniforge3/envs/tensorflow/lib/python3.8/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c106detail19maybe_wrap_dim_slowExxb\n",
      "  Referenced from: <8E58E83E-9235-3324-9B6B-260614F85F69> /Users/tglim/miniforge3/envs/tensorflow/lib/python3.8/site-packages/torchvision/image.so\n",
      "  Expected in:     <3F9923D2-81A5-3EC8-9739-EC0C1C816132> /Users/tglim/miniforge3/envs/tensorflow/lib/python3.8/site-packages/torch/lib/libc10.dylib\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "from gensim.models import word2vec\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from python_scripts import data_setup, engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_class = {\n",
    "  0: 'Origin(출신차별)',\n",
    "  1: 'Physical(외모차별) 외모(신체, 얼굴) 및 장애인 차별 발언을 포함합니다.',\n",
    "  2: 'Politics(정치성향차별)',\n",
    "  3: 'Profanity(혐오욕설) 욕설,저주,혐오 단어, 비속어 및 기타 혐오 발언을 포함합니다.',\n",
    "  4: 'Age(연령차별)',\n",
    "  5: 'Gender(성차별) 성별 또는 성적 취향에 대한 차별 발언을 포함합니다.',\n",
    "  6: 'Not Hate Speech(해당사항없음)',\n",
    "}\n",
    "\n",
    "max_length = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>유소영비호감 성형아줌마</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>나오지마라 썅</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>식상하고 지긋지긋했는데 잘 끝나네 오예 소리벗고 빤스질러~~!!!</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>성희롱 당할 얼굴이 아닌데?ㅋㅋㅋ</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>\"끝까지 해보자~쪽파리 원숭이 자한 쓰레기당\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65858</th>\n",
       "      <td>65858</td>\n",
       "      <td>ㅋ ㅋ 쇼~~~ 도 적당히</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65859</th>\n",
       "      <td>65859</td>\n",
       "      <td>\"이젠 전라도 종것들 음식 불매다. 태양광 중금속 환경오염물로 만든 식품 사절이다\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65860</th>\n",
       "      <td>65860</td>\n",
       "      <td>조센징들은 참 피곤하게 산다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65861</th>\n",
       "      <td>65861</td>\n",
       "      <td>\"문빠 다모아서 빨갱이한테보내고 행복하게살라고\"</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65862</th>\n",
       "      <td>65862</td>\n",
       "      <td>항마력 딸려서 못보겟음 보는사람들 대부분 환상쩌는여자일듯..</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65863 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                                            text  label\n",
       "0          0                                    유소영비호감 성형아줌마      1\n",
       "1          1                                         나오지마라 썅      3\n",
       "2          2            식상하고 지긋지긋했는데 잘 끝나네 오예 소리벗고 빤스질러~~!!!      6\n",
       "3          3                              성희롱 당할 얼굴이 아닌데?ㅋㅋㅋ      5\n",
       "4          4                       \"끝까지 해보자~쪽파리 원숭이 자한 쓰레기당\"      0\n",
       "...      ...                                             ...    ...\n",
       "65858  65858                                  ㅋ ㅋ 쇼~~~ 도 적당히      6\n",
       "65859  65859  \"이젠 전라도 종것들 음식 불매다. 태양광 중금속 환경오염물로 만든 식품 사절이다\"      0\n",
       "65860  65860                                 조센징들은 참 피곤하게 산다      0\n",
       "65861  65861                      \"문빠 다모아서 빨갱이한테보내고 행복하게살라고\"      2\n",
       "65862  65862               항마력 딸려서 못보겟음 보는사람들 대부분 환상쩌는여자일듯..      5\n",
       "\n",
       "[65863 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('../data/Discrimination/train.csv')\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "306"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(s) for s in train_data['text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_korean_text(text):\n",
    "    # Remove URLs and mentions <= 본 문제에서는 url 이나 email 주소가 나오지 않으니 그리 크게 중요하지 않을듯\n",
    "    text = re.sub(r\"(http|https)?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\", \"\", text)\n",
    "    text = re.sub(r\"@(\\w+)\", \"\", text)\n",
    "\n",
    "    # Tokenize text using Mecab <= 이 부분은 다른 라이브러리로 바꾸어도 무방함\n",
    "    mecab = Mecab()\n",
    "    tokens = mecab.morphs(text)\n",
    "\n",
    "    # Remove stop words (optional) <= 필요하다면 바꾸어 보아도 됨\n",
    "    stop_words = [\"은\", \"는\", \"이\", \"가\", \"을\", \"를\", \"에\", \"의\", \"로\", \"으로\", \"에서\"]\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "\n",
    "    # Remove punctuation and non-Korean characters <= 필요하다면 바꾸어 보아도 됨\n",
    "    tokens = [re.sub(r\"[^\\u3131-\\u3163\\uac00-\\ud7a3]+\", \"\", t) for t in tokens]\n",
    "    tokens = [t for t in tokens if t]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['나', '지금', '뭐', '하', '고', '있', '느냐']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_korean_text('나는 지금 뭐하고 있느냐?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Getting Word2Vec embedding pre-trained model <= 1번 word-embedding model\n",
    "\n",
    "# w2v_pretrained_model = word2vec.Word2Vec.load('../data/Discrimination/word2vec')\n",
    "# w2v_pretrained_model.wv.add_vector('<unk>', [0.0] * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Getting GloVe embedding pre-trained model <= 2번 word-embedding model\n",
    "\n",
    "# def load_glove_model(file):\n",
    "#     print(\"Loading Glove Weight\")\n",
    "#     glove_vector = {}\n",
    "#     with open(file,'r') as f:\n",
    "#         for line in f:\n",
    "#             split_line = line.split()\n",
    "#             word = split_line[0]\n",
    "#             embedding = np.array(split_line[1:], dtype=np.float64)\n",
    "#             glove_vector[word] = embedding\n",
    "\n",
    "#     # 없는 단어 추가 <= 아래 ref 1. 성능 변화 미미함\n",
    "#     not_exists = pd.read_csv('../data/Discrimination/not_exist_list.csv')\n",
    "#     for i in range(len(not_exists)):\n",
    "#         glove_vector[not_exists.loc[i, 'word']] = glove_vector[not_exists.loc[i, 'substitute']]\n",
    "\n",
    "#     class Word_vector():\n",
    "#         def __init__(self, key_to_vector) -> None:\n",
    "#             self.key_to_vector = key_to_vector\n",
    "\n",
    "#             self.index_to_key = []\n",
    "#             self.key_to_index = {}\n",
    "#             for key in self.key_to_vector.keys():\n",
    "#                 self.index_to_key.append(key)\n",
    "#                 self.key_to_index[key] = len(self.index_to_key) - 1\n",
    "\n",
    "#             self.vectors = []\n",
    "#             for i in range(len(self.index_to_key)):\n",
    "#                 self.vectors.append(self.key_to_vector[self.index_to_key[i]])\n",
    "#             self.vectors = np.array(self.vectors, dtype='float32')\n",
    "\n",
    "#             self.vector_size = len(self.vectors[0])\n",
    "\n",
    "#         def __contains__(self, key):\n",
    "#             return key in self.key_to_vector\n",
    "\n",
    "#         def __getitem__(self, key):\n",
    "#             return self.key_to_vector[key]\n",
    "\n",
    "#         def __len__(self):\n",
    "#             return len(self.index_to_key)\n",
    "\n",
    "#     class Glove_model():\n",
    "#         def __init__(self, vector) -> None:\n",
    "#             self.wv = Word_vector(vector)\n",
    "\n",
    "#         def __len__(self):\n",
    "#             return len(self.wv)\n",
    "\n",
    "#     glove_model = Glove_model(glove_vector)\n",
    "\n",
    "#     print(f\"{len(glove_model)} words loaded!\")\n",
    "#     return glove_model\n",
    "\n",
    "# glove_pretrained_model = load_glove_model('../data/Discrimination/glove.txt')\n",
    "# len(glove_pretrained_model.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref 1. pre-trained Glove 임베딩 벡터에 등록되지 않은 단어 중 주요 단어를 골라 수작업 처리 <= 성능 영향 미미함\n",
    "# all = 0\n",
    "# not_exists = {}\n",
    "# not_exists_labels = {}\n",
    "\n",
    "# for i in tqdm_notebook(range(len(train_data))):\n",
    "#     sentence, label = train_data.loc[i, 'text'], train_data.loc[i, 'label']\n",
    "#     for word in preprocess_korean_text(sentence):\n",
    "#         if word not in glove_pretrained_model.wv:\n",
    "#             if word in not_exists:\n",
    "#                 not_exists[word] += 1\n",
    "#             else:\n",
    "#                 not_exists[word] = 1\n",
    "\n",
    "#             if word not in not_exists_labels:\n",
    "#                 not_exists_labels[word] = {n: 0 for n in range(7)}\n",
    "#             not_exists_labels[word][label] += 1\n",
    "#         all += 1\n",
    "\n",
    "# not_exists = sorted(not_exists.items(), key=lambda x: x[1], reverse=True)\n",
    "# not_labels = []\n",
    "# for word, n in not_exists:\n",
    "#     not_labels.append(sorted(not_exists_labels[word].items(), key=lambda x: x[1], reverse=True))\n",
    "# print(all, len(not_exists))\n",
    "# print(not_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not_exist_list = pd.DataFrame({'word': not_exists, 'label': not_labels})\n",
    "# not_exist_list.to_csv('../data/Discrimination/not_exist_list.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05563ceb9757446a8b0cd1e6ad4f1b12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Making word maps:   0%|          | 0/65863 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "word_index_to_key = []\n",
    "word_key_to_index = {}\n",
    "\n",
    "for i in tqdm_notebook(range(len(train_data)), 'Making word maps'):\n",
    "    text = train_data.iloc[i]['text']\n",
    "    tokens = preprocess_korean_text(text)\n",
    "\n",
    "    for token in tokens:\n",
    "        if token not in word_key_to_index:\n",
    "            word_key_to_index[token] = len(word_index_to_key)\n",
    "            word_index_to_key.append(token)\n",
    "\n",
    "word_key_to_index['<unk>'] = len(word_index_to_key)\n",
    "word_index_to_key.append('<unk>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KoreanTextDataset(Dataset):\n",
    "    def __init__(self, data, preprocess_korean_text, max_length=100):\n",
    "        self.data = data\n",
    "        self.max_length = max_length\n",
    "        self.preprocess_korean_text = preprocess_korean_text\n",
    "        self.idx_to_class = sorted(data['label'].unique())\n",
    "        self.class_to_idx = {}\n",
    "        for i in range(len(self.idx_to_class)):\n",
    "            self.class_to_idx[self.idx_to_class[i]] = i\n",
    "        self.class_names = self.idx_to_class\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.data.loc[index, \"text\"]\n",
    "        label = self.data.loc[index, \"label\"]\n",
    "\n",
    "        # Preprocess text using the preprocess_korean_text() function\n",
    "        tokens = self.preprocess_korean_text(text)\n",
    "        # Truncate or pad tokens to a fixed length\n",
    "        if len(tokens) > self.max_length:\n",
    "            tokens = tokens[:self.max_length]\n",
    "        else:\n",
    "            tokens += [\"\"] * (self.max_length - len(tokens))\n",
    "\n",
    "        # Convert tokens to indices using the pre-trained GloVe or Word2Vec embeddings\n",
    "        indices = []\n",
    "        for token in tokens:\n",
    "            if token in word_key_to_index:\n",
    "                indices.append(word_key_to_index[token])\n",
    "            else:\n",
    "                indices.append(word_key_to_index['<unk>'])  # use the index of the <unk> token for out-of-vocabulary words\n",
    "\n",
    "        return torch.tensor(indices), torch.tensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Splitting dataset of length 65863 into splits of size: 59276 and 6587\n"
     ]
    }
   ],
   "source": [
    "train_dataset = KoreanTextDataset(\n",
    "    data=train_data,\n",
    "    preprocess_korean_text=preprocess_korean_text,\n",
    "    max_length=max_length\n",
    ")\n",
    "\n",
    "train_dataset_sub, val_dataset_sub = data_setup.split_dataset(\n",
    "    dataset=train_dataset,\n",
    "    split_size=0.9,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "        self.v = nn.Linear(hidden_size, 1, bias=False)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        max_len = encoder_outputs.size(0)\n",
    "\n",
    "        repeated_hidden = hidden.unsqueeze(0).repeat(max_len, 1, 1)\n",
    "\n",
    "        energy = torch.tanh(self.attn(torch.cat((repeated_hidden, encoder_outputs), dim=2)))\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        return F.softmax(attention, dim=0).unsqueeze(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_LSTM_attention(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, pre_LSTM_layers, post_LSTM_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        for param in self.embedding.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.pre_lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=pre_LSTM_layers, bidirectional=True, dropout=dropout)\n",
    "        self.post_lstm = nn.LSTM(hidden_dim * 2, hidden_dim, num_layers=post_LSTM_layers, bidirectional=True, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attention = Attention(hidden_dim * 2)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, text):\n",
    "        # text = [batch size, seq len]\n",
    "\n",
    "        embedded = self.embedding(text)\n",
    "        embedded = self.dropout(embedded)\n",
    "        # embedded = [batch size, seq len, emb dim]: [64, 200, 100]\n",
    "        # print('embedded: ', embedded.shape)\n",
    "\n",
    "        pre_lstm_outputs, (hidden, cell) = self.pre_lstm(embedded.permute(1, 0, 2))\n",
    "        # output = [batch size, seq len, hid dim * num directions]: [200, 64, 1024]\n",
    "        # hidden/cell = [num layers * num directions, batch size, hid dim]: [6, 64, 512]\n",
    "        # print('outputs, hidden: ', pre_lstm_outputs.shape, hidden.shape)\n",
    "\n",
    "        h = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
    "        # [64, 1024]\n",
    "        # print('h: ', h.shape)\n",
    "\n",
    "        attention_weights = self.attention(h, pre_lstm_outputs)\n",
    "        # # attention_weights = [batch size, seq len, 1]: [200, 64, 1]\n",
    "        # print('attention_weights: ', attention_weights.shape)\n",
    "\n",
    "        context_vector = torch.bmm(pre_lstm_outputs.permute(1, 2, 0), attention_weights.permute(1, 0, 2)).squeeze(2)\n",
    "        # # context_vector = [batch size, hid dim * num directions]: [64, 1024]\n",
    "        # print('context_vector: ', context_vector.shape)\n",
    "\n",
    "        _, (hidden, _) = self.post_lstm(context_vector.unsqueeze(0), (hidden, cell))\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        # hidden = [batch size, hid dim * num directions]: [64, 1024]\n",
    "        # print('hidden: ', hidden.shape)\n",
    "\n",
    "        out = self.fc(self.dropout(hidden.squeeze(0)))\n",
    "        # out = [batch size, output dim]: [64, 7]\n",
    "        # print('out: ', out.shape)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_list = [1e-3] # 각 LR 별로 10 epoch 씩 연달아 학습 진행\n",
    "weight_decay_list = [1e-4]\n",
    "epochs_list = [5]\n",
    "batch_size_list = [64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 1, 2, 3, 4, 5, 6], 7)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names, num_classes = train_dataset.class_names, len(train_dataset.class_names)\n",
    "class_names, num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN_LSTM_attention(\n",
    "    vocab_size=len(word_index_to_key),\n",
    "    embedding_dim=100,\n",
    "    hidden_dim=512,\n",
    "    output_dim=num_classes,\n",
    "    pre_LSTM_layers=2,\n",
    "    post_LSTM_layers=2,\n",
    "    dropout=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "RNN_LSTM_attention                       --\n",
       "├─Embedding: 1-1                         (3,674,200)\n",
       "├─LSTM: 1-2                              8,814,592\n",
       "├─LSTM: 1-3                              12,599,296\n",
       "├─Dropout: 1-4                           --\n",
       "├─Attention: 1-5                         --\n",
       "│    └─Linear: 2-1                       2,098,176\n",
       "│    └─Linear: 2-2                       1,024\n",
       "├─Linear: 1-6                            7,175\n",
       "=================================================================\n",
       "Total params: 27,194,463\n",
       "Trainable params: 23,520,263\n",
       "Non-trainable params: 3,674,200\n",
       "================================================================="
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69f0c4f178e6422aa2c754fe5a4e9f93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Two_LSTM_attention_discrimination_LR_0.001_WD_0.0001_BS_64_GA_1:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "833505053c104e03bf4aa8c3a067640f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/927 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/tglim/Kaggle/notebooks/discrimination.ipynb Cell 22\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#Y266sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m tuning_results \u001b[39m=\u001b[39m engine\u001b[39m.\u001b[39;49mHP_tune_train(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#Y266sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#Y266sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     model_generator\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#Y266sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     model_weights\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#Y266sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     model_name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mTwo_LSTM_attention_discrimination\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#Y266sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     train_dataset\u001b[39m=\u001b[39;49mtrain_dataset_sub,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#Y266sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     test_dataset\u001b[39m=\u001b[39;49mval_dataset_sub,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#Y266sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     class_names\u001b[39m=\u001b[39;49mclass_names,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#Y266sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     learning_rate_list\u001b[39m=\u001b[39;49mlearning_rate_list,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#Y266sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     weight_decay_list\u001b[39m=\u001b[39;49mweight_decay_list,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#Y266sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     epochs_list\u001b[39m=\u001b[39;49mepochs_list,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#Y266sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     batch_size_list\u001b[39m=\u001b[39;49mbatch_size_list,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#Y266sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     is_tensorboard_writer\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#Y266sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#Y266sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     gradient_accumulation_num\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#Y266sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     saving_max\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#Y266sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     metric_learning\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#Y266sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m )\n",
      "File \u001b[0;32m~/Kaggle/notebooks/../python_scripts/engine.py:263\u001b[0m, in \u001b[0;36mHP_tune_train\u001b[0;34m(model, model_generator, model_weights, model_name, train_dataset, test_dataset, class_names, learning_rate_list, weight_decay_list, epochs_list, batch_size_list, is_tensorboard_writer, device, gradient_accumulation_num, saving_max, metric_learning)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[39mif\u001b[39;00m is_tensorboard_writer:\n\u001b[1;32m    257\u001b[0m     writer \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mcreate_writer(\n\u001b[1;32m    258\u001b[0m         experiment_name\u001b[39m=\u001b[39m model_name \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_test\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m    259\u001b[0m         model_name\u001b[39m=\u001b[39mmodel_name,\n\u001b[1;32m    260\u001b[0m         extra\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mLR_\u001b[39m\u001b[39m{\u001b[39;00mlearning_rate\u001b[39m}\u001b[39;00m\u001b[39m_WD_\u001b[39m\u001b[39m{\u001b[39;00mweight_decay\u001b[39m}\u001b[39;00m\u001b[39m_EP_\u001b[39m\u001b[39m{\u001b[39;00mepochs\u001b[39m}\u001b[39;00m\u001b[39m_BS_\u001b[39m\u001b[39m{\u001b[39;00mbatch_size\u001b[39m}\u001b[39;00m\u001b[39m_GA_\u001b[39m\u001b[39m{\u001b[39;00mgradient_accumulation_num\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m    261\u001b[0m     )\n\u001b[0;32m--> 263\u001b[0m model_results \u001b[39m=\u001b[39m train_tensorboard_gradient_accumulation(\n\u001b[1;32m    264\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m    265\u001b[0m     save_name\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00mmodel_name\u001b[39m}\u001b[39;49;00m\u001b[39m_LR_\u001b[39;49m\u001b[39m{\u001b[39;49;00mlearning_rate\u001b[39m}\u001b[39;49;00m\u001b[39m_WD_\u001b[39;49m\u001b[39m{\u001b[39;49;00mweight_decay\u001b[39m}\u001b[39;49;00m\u001b[39m_BS_\u001b[39;49m\u001b[39m{\u001b[39;49;00mbatch_size\u001b[39m}\u001b[39;49;00m\u001b[39m_GA_\u001b[39;49m\u001b[39m{\u001b[39;49;00mgradient_accumulation_num\u001b[39m}\u001b[39;49;00m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m    266\u001b[0m     train_dataloader\u001b[39m=\u001b[39;49mtrain_dataloader,\n\u001b[1;32m    267\u001b[0m     test_dataloader\u001b[39m=\u001b[39;49mtest_dataloader,\n\u001b[1;32m    268\u001b[0m     loss_fn\u001b[39m=\u001b[39;49mnn\u001b[39m.\u001b[39;49mCrossEntropyLoss(),\n\u001b[1;32m    269\u001b[0m     optimizer\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49moptim\u001b[39m.\u001b[39;49mAdam(\n\u001b[1;32m    270\u001b[0m         params\u001b[39m=\u001b[39;49mmodel\u001b[39m.\u001b[39;49mparameters(),\n\u001b[1;32m    271\u001b[0m         lr\u001b[39m=\u001b[39;49mlearning_rate,\n\u001b[1;32m    272\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mweight_decay\n\u001b[1;32m    273\u001b[0m     ),\n\u001b[1;32m    274\u001b[0m     accuracy_fn\u001b[39m=\u001b[39;49mAccuracy(\n\u001b[1;32m    275\u001b[0m         task\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmulticlass\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m    276\u001b[0m         num_classes\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(class_names)\n\u001b[1;32m    277\u001b[0m     ),\n\u001b[1;32m    278\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[1;32m    279\u001b[0m     device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m    280\u001b[0m     writer\u001b[39m=\u001b[39;49mwriter,\n\u001b[1;32m    281\u001b[0m     accumulation_num\u001b[39m=\u001b[39;49mgradient_accumulation_num,\n\u001b[1;32m    282\u001b[0m     saving_max\u001b[39m=\u001b[39;49msaving_max,\n\u001b[1;32m    283\u001b[0m     metric_learning\u001b[39m=\u001b[39;49mmetric_learning\n\u001b[1;32m    284\u001b[0m )\n\u001b[1;32m    286\u001b[0m t_result[\u001b[39m'\u001b[39m\u001b[39mtrain_loss\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m model_results[\u001b[39m'\u001b[39m\u001b[39mtrain_loss\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    287\u001b[0m t_result[\u001b[39m'\u001b[39m\u001b[39mtest_loss\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m model_results[\u001b[39m'\u001b[39m\u001b[39mtest_loss\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/Kaggle/notebooks/../python_scripts/engine.py:154\u001b[0m, in \u001b[0;36mtrain_tensorboard_gradient_accumulation\u001b[0;34m(model, save_name, train_dataloader, test_dataloader, loss_fn, optimizer, accuracy_fn, epochs, device, writer, accumulation_num, saving_max, metric_learning)\u001b[0m\n\u001b[1;32m    151\u001b[0m model\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m    153\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tqdm_notebook(\u001b[39mrange\u001b[39m(epochs), desc\u001b[39m=\u001b[39msave_name, leave\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m--> 154\u001b[0m     train_loss, train_acc \u001b[39m=\u001b[39m train_step_gradient_accumulation(model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m    155\u001b[0m                                        dataloader\u001b[39m=\u001b[39;49mtrain_dataloader,\n\u001b[1;32m    156\u001b[0m                                        loss_fn\u001b[39m=\u001b[39;49mloss_fn,\n\u001b[1;32m    157\u001b[0m                                        optimizer\u001b[39m=\u001b[39;49moptimizer,\n\u001b[1;32m    158\u001b[0m                                        accuracy_fn\u001b[39m=\u001b[39;49maccuracy_fn,\n\u001b[1;32m    159\u001b[0m                                        device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m    160\u001b[0m                                        accumulation_num\u001b[39m=\u001b[39;49maccumulation_num,\n\u001b[1;32m    161\u001b[0m                                        metric_learning\u001b[39m=\u001b[39;49mmetric_learning)\n\u001b[1;32m    162\u001b[0m     test_loss, test_acc \u001b[39m=\u001b[39m test_step(model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m    163\u001b[0m                                     dataloader\u001b[39m=\u001b[39mtest_dataloader,\n\u001b[1;32m    164\u001b[0m                                     loss_fn\u001b[39m=\u001b[39mloss_fn,\n\u001b[1;32m    165\u001b[0m                                     accuracy_fn\u001b[39m=\u001b[39maccuracy_fn,\n\u001b[1;32m    166\u001b[0m                                     device\u001b[39m=\u001b[39mdevice,\n\u001b[1;32m    167\u001b[0m                                     metric_learning\u001b[39m=\u001b[39mmetric_learning)\n\u001b[1;32m    169\u001b[0m     results[\u001b[39m'\u001b[39m\u001b[39mtrain_loss\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mappend(train_loss\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy())\n",
      "File \u001b[0;32m~/Kaggle/notebooks/../python_scripts/engine.py:43\u001b[0m, in \u001b[0;36mtrain_step_gradient_accumulation\u001b[0;34m(model, dataloader, loss_fn, optimizer, accuracy_fn, device, accumulation_num, metric_learning)\u001b[0m\n\u001b[1;32m     41\u001b[0m   y_logits \u001b[39m=\u001b[39m model(X_batch_train, y_batch_train)\n\u001b[1;32m     42\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 43\u001b[0m   y_logits \u001b[39m=\u001b[39m model(X_batch_train)\n\u001b[1;32m     44\u001b[0m y_pred \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(y_logits, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     46\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(y_logits, y_batch_train) \u001b[39m/\u001b[39m accumulation_num\n",
      "File \u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/tglim/Kaggle/notebooks/discrimination.ipynb Cell 22\u001b[0m in \u001b[0;36mRNN_LSTM_attention.forward\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#Y266sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m embedded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(embedded)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#Y266sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# embedded = [batch size, seq len, emb dim]: [64, 200, 100]\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#Y266sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# print('embedded: ', embedded.shape)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#Y266sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m pre_lstm_outputs, (hidden, cell) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpre_lstm(embedded\u001b[39m.\u001b[39;49mpermute(\u001b[39m1\u001b[39;49m, \u001b[39m0\u001b[39;49m, \u001b[39m2\u001b[39;49m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#Y266sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# output = [batch size, seq len, hid dim * num directions]: [200, 64, 1024]\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#Y266sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# hidden/cell = [num layers * num directions, batch size, hid dim]: [6, 64, 512]\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#Y266sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# print('outputs, hidden: ', pre_lstm_outputs.shape, hidden.shape)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#Y266sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m h \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((hidden[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m,:,:], hidden[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,:,:]), dim \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.8/site-packages/torch/nn/modules/rnn.py:812\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_forward_args(\u001b[39minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m    811\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 812\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[1;32m    813\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[1;32m    814\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    815\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[1;32m    816\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tuning_results = engine.HP_tune_train(\n",
    "    model=model,\n",
    "    model_generator=None,\n",
    "    model_weights=None,\n",
    "    model_name='Two_LSTM_attention_discrimination',\n",
    "    train_dataset=train_dataset_sub,\n",
    "    test_dataset=val_dataset_sub,\n",
    "    class_names=class_names,\n",
    "    learning_rate_list=learning_rate_list,\n",
    "    weight_decay_list=weight_decay_list,\n",
    "    epochs_list=epochs_list,\n",
    "    batch_size_list=batch_size_list,\n",
    "    is_tensorboard_writer=False,\n",
    "    device=device,\n",
    "    gradient_accumulation_num=1,\n",
    "    saving_max=False,\n",
    "    metric_learning=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_weight = torch.load('..\\models\\discrimination\\LSTM_attention_glove_discrimination_LR_0.0001_WD_0.0001_BS_64_GA_1_EPOCH_0_TEST-ACC_0.7751.pth')\n",
    "model.load_state_dict(loaded_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('../data/Discrimination/test.csv')\n",
    "labels = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.inference_mode():\n",
    "  for i in tqdm_notebook(range(len(test_data))):\n",
    "    test_text = test_data.loc[i, \"text\"]\n",
    "    test_tokens = preprocess_korean_text(test_text)\n",
    "    if len(test_tokens) > max_length:\n",
    "        test_tokens = test_tokens[:max_length]\n",
    "    else:\n",
    "        test_tokens += [\"\"] * (max_length - len(test_tokens))\n",
    "\n",
    "    indices = []\n",
    "    for token in test_tokens:\n",
    "      if token in word_key_to_index:\n",
    "        indices.append(word_key_to_index[token])\n",
    "      else:\n",
    "        indices.append(word_key_to_index['<unk>'])  # use the index of the <unk> token for out-of-vocabulary words\n",
    "\n",
    "    test_logits = model(torch.tensor(indices).unsqueeze(0).to(device))\n",
    "    labels.append(class_names[torch.argmax(test_logits.squeeze(0).cpu())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['label'] = [idx_to_class[label] for label in labels]\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_data = pd.DataFrame({'ID': range(len(test_data)), 'label': labels})\n",
    "submission_data.to_csv('../submissions/discrimination/submission.csv', index=False)\n",
    "print('submission completed!')\n",
    "submission_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
