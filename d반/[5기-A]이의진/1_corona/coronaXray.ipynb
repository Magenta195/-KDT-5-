{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in /Users/lee/kdt-ai-course/KDT_AI/.conda/lib/python3.10/site-packages (0.15.1)\n",
      "Requirement already satisfied: requests in /Users/lee/kdt-ai-course/KDT_AI/.conda/lib/python3.10/site-packages (from torchvision) (2.28.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/lee/kdt-ai-course/KDT_AI/.conda/lib/python3.10/site-packages (from torchvision) (9.4.0)\n",
      "Requirement already satisfied: torch==2.0.0 in /Users/lee/kdt-ai-course/KDT_AI/.conda/lib/python3.10/site-packages (from torchvision) (2.0.0)\n",
      "Requirement already satisfied: numpy in /Users/lee/kdt-ai-course/KDT_AI/.conda/lib/python3.10/site-packages (from torchvision) (1.24.2)\n",
      "Requirement already satisfied: jinja2 in /Users/lee/kdt-ai-course/KDT_AI/.conda/lib/python3.10/site-packages (from torch==2.0.0->torchvision) (3.1.2)\n",
      "Requirement already satisfied: typing-extensions in /Users/lee/kdt-ai-course/KDT_AI/.conda/lib/python3.10/site-packages (from torch==2.0.0->torchvision) (4.5.0)\n",
      "Requirement already satisfied: sympy in /Users/lee/kdt-ai-course/KDT_AI/.conda/lib/python3.10/site-packages (from torch==2.0.0->torchvision) (1.11.1)\n",
      "Requirement already satisfied: filelock in /Users/lee/kdt-ai-course/KDT_AI/.conda/lib/python3.10/site-packages (from torch==2.0.0->torchvision) (3.12.0)\n",
      "Requirement already satisfied: networkx in /Users/lee/kdt-ai-course/KDT_AI/.conda/lib/python3.10/site-packages (from torch==2.0.0->torchvision) (3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/lee/kdt-ai-course/KDT_AI/.conda/lib/python3.10/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/lee/kdt-ai-course/KDT_AI/.conda/lib/python3.10/site-packages (from requests->torchvision) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/lee/kdt-ai-course/KDT_AI/.conda/lib/python3.10/site-packages (from requests->torchvision) (3.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/lee/kdt-ai-course/KDT_AI/.conda/lib/python3.10/site-packages (from requests->torchvision) (1.26.15)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/lee/kdt-ai-course/KDT_AI/.conda/lib/python3.10/site-packages (from jinja2->torch==2.0.0->torchvision) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/lee/kdt-ai-course/KDT_AI/.conda/lib/python3.10/site-packages (from sympy->torch==2.0.0->torchvision) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch in /Users/lee/kdt-ai-course/KDT_AI/.conda/lib/python3.10/site-packages (2.0.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/lee/kdt-ai-course/KDT_AI/.conda/lib/python3.10/site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: networkx in /Users/lee/kdt-ai-course/KDT_AI/.conda/lib/python3.10/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: sympy in /Users/lee/kdt-ai-course/KDT_AI/.conda/lib/python3.10/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: jinja2 in /Users/lee/kdt-ai-course/KDT_AI/.conda/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: filelock in /Users/lee/kdt-ai-course/KDT_AI/.conda/lib/python3.10/site-packages (from torch) (3.12.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/lee/kdt-ai-course/KDT_AI/.conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/lee/kdt-ai-course/KDT_AI/.conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in /Users/lee/kdt-ai-course/KDT_AI/.conda/lib/python3.10/site-packages (1.5.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/lee/kdt-ai-course/KDT_AI/.conda/lib/python3.10/site-packages (from pandas) (1.24.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/lee/kdt-ai-course/KDT_AI/.conda/lib/python3.10/site-packages (from pandas) (2022.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/lee/kdt-ai-course/KDT_AI/.conda/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/lee/kdt-ai-course/KDT_AI/.conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torchvision\n",
    "%pip install torch\n",
    "%pip install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "\n",
    "class Dataset_type(Enum):\n",
    "    TRAIN = 0\n",
    "    TEST = 1\n",
    "\n",
    "\n",
    "class CovidDataset(Dataset):\n",
    "    def __init__(self, src_path, trg_file, dataset_type: Dataset_type, transform=None):\n",
    "        super().__init__()\n",
    "        self.src_path = src_path\n",
    "        self.transform = transform\n",
    "        self.dataset_type = dataset_type\n",
    "        self.trg_df = pd.read_csv(trg_file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.trg_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.trg_df.iloc[idx, 0]\n",
    "        src = Image.open(os.path.join(self.src_path, filename))\n",
    "        src = self.transform(src) if self.transform is not None else src\n",
    "\n",
    "        if self.dataset_type == Dataset_type.TRAIN:\n",
    "            trg = torch.tensor(\n",
    "                1 if self.trg_df.iloc[idx, 1] == \"covid\" else 0)\n",
    "\n",
    "            return src, trg\n",
    "        else:\n",
    "            return src\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/lee/kdt-ai-course/competition/KDT_AI_project_competition_study/d반/[5기-A]이의진/1_corona/train'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_path = os.getcwd()\n",
    "img_path = os.path.join(current_path, 'train')\n",
    "img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800\n",
      "200\n",
      "400\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "current_path = os.getcwd()\n",
    "train_path = os.path.join(current_path, \"dataset\", \"train\")\n",
    "train_trg_file = os.path.join(train_path, \"labels.csv\")\n",
    "test_path = os.path.join(current_path, \"dataset\", \"test\")\n",
    "test_trg_file = os.path.join(test_path, \"submission.csv\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (1,))\n",
    "])\n",
    "\n",
    "train_set = CovidDataset(src_path=train_path, trg_file=train_trg_file,\n",
    "                         dataset_type=Dataset_type.TRAIN, transform=transform)\n",
    "test_set = CovidDataset(src_path=test_path, trg_file=test_trg_file,\n",
    "                        dataset_type=Dataset_type.TEST, transform=transform)\n",
    "\n",
    "train_valid_ratio = 0.9\n",
    "train_set_count = int(len(train_set) * train_valid_ratio)\n",
    "val_set_count = len(train_set) - train_set_count\n",
    "train_set, val_set = random_split(train_set, [train_set_count, val_set_count])\n",
    "print(len(train_set))\n",
    "print(len(val_set))\n",
    "print(len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.3902, -0.4333, -0.4686,  ..., -0.2098,  0.0137,  0.1863],\n",
       "          [-0.4765, -0.4882, -0.4961,  ..., -0.3667, -0.2020, -0.0333],\n",
       "          [-0.5000, -0.5000, -0.5000,  ..., -0.4569, -0.3471, -0.2137],\n",
       "          ...,\n",
       "          [-0.4961, -0.5000, -0.5000,  ..., -0.1784, -0.1549, -0.1549],\n",
       "          [-0.4922, -0.5000, -0.5000,  ..., -0.1431, -0.1235, -0.1314],\n",
       "          [-0.4882, -0.4961, -0.5000,  ..., -0.1118, -0.0922, -0.1118]]]),\n",
       " tensor(0))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# 데이터로더를 만듭니다.\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Conv_Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
    "        self.maxpool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.fc1 = nn.Linear(64*74*74, 128)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = x.view(-1, 64 * 74 * 74)  # 입력 데이터를 펼칩니다.\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "\n",
    "model = Conv_Net().to(device)\n",
    "\n",
    "# 손실 함수와 최적화 알고리즘을 정의합니다.\n",
    "criterion = nn.BCELoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "def train(num_of_epoch):\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    for epoch in range(num_of_epoch):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            import sys\n",
    "\n",
    "            # print(f\"target: {target.shape}\")\n",
    "            # print(f\"output: {output.shape}\")\n",
    "            output = torch.squeeze(output, dim=1)\n",
    "            output = output.to(torch.float32)\n",
    "            target = target.to(torch.float32)\n",
    "            # print(f\"target: {target.shape}\")\n",
    "            # print(f\"output: {output.shape}\")\n",
    "            # print(target)\n",
    "            # print(output)\n",
    "            # sys.exit()\n",
    "            loss = criterion(output, target)\n",
    "            # sys.exit()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            pred = torch.tensor(\n",
    "                [1 if output_data >= 0.5 else 0 for output_data in output]).to(device)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "            if batch_idx % 5 == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch + 1, batch_idx *\n",
    "                    len(data), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "    train_accuracy = 100. * (correct / len(train_set))\n",
    "    train_losses.append(loss.item())\n",
    "    train_accs.append(train_accuracy)\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            output = torch.squeeze(output, dim=1)\n",
    "            output = output.to(torch.float32)\n",
    "            target = target.to(torch.float32)\n",
    "            loss = criterion(output, target)\n",
    "            pred = torch.tensor(\n",
    "                [1 if output_data >= 0.5 else 0 for output_data in output]).to(device)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        val_accuracy = 100. * (correct / len(val_set))\n",
    "        val_losses.append(loss.item())\n",
    "        val_accs.append(val_accuracy)\n",
    "\n",
    "        print('Epoch {} finished: train loss = {}, val loss = {}'.format(epoch + 1,\n",
    "                                                                         train_losses[-1], val_losses[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/1800 (0%)]\tLoss: 0.691277\n",
      "Train Epoch: 1 [160/1800 (9%)]\tLoss: 0.885873\n",
      "Train Epoch: 1 [320/1800 (18%)]\tLoss: 0.679173\n",
      "Train Epoch: 1 [480/1800 (26%)]\tLoss: 0.684840\n",
      "Train Epoch: 1 [640/1800 (35%)]\tLoss: 0.692103\n",
      "Train Epoch: 1 [800/1800 (44%)]\tLoss: 0.694850\n",
      "Train Epoch: 1 [960/1800 (53%)]\tLoss: 0.693868\n",
      "Train Epoch: 1 [1120/1800 (61%)]\tLoss: 0.595851\n",
      "Train Epoch: 1 [1280/1800 (70%)]\tLoss: 0.542146\n",
      "Train Epoch: 1 [1440/1800 (79%)]\tLoss: 0.592285\n",
      "Train Epoch: 1 [1600/1800 (88%)]\tLoss: 0.499671\n",
      "Train Epoch: 1 [1760/1800 (96%)]\tLoss: 0.505445\n",
      "Train Epoch: 2 [0/1800 (0%)]\tLoss: 0.618874\n",
      "Train Epoch: 2 [160/1800 (9%)]\tLoss: 0.477265\n",
      "Train Epoch: 2 [320/1800 (18%)]\tLoss: 0.419029\n",
      "Train Epoch: 2 [480/1800 (26%)]\tLoss: 0.599740\n",
      "Train Epoch: 2 [640/1800 (35%)]\tLoss: 0.492073\n",
      "Train Epoch: 2 [800/1800 (44%)]\tLoss: 0.440180\n",
      "Train Epoch: 2 [960/1800 (53%)]\tLoss: 0.507038\n",
      "Train Epoch: 2 [1120/1800 (61%)]\tLoss: 0.510532\n",
      "Train Epoch: 2 [1280/1800 (70%)]\tLoss: 0.493531\n",
      "Train Epoch: 2 [1440/1800 (79%)]\tLoss: 0.279905\n",
      "Train Epoch: 2 [1600/1800 (88%)]\tLoss: 0.521094\n",
      "Train Epoch: 2 [1760/1800 (96%)]\tLoss: 0.431448\n",
      "Train Epoch: 3 [0/1800 (0%)]\tLoss: 0.372276\n",
      "Train Epoch: 3 [160/1800 (9%)]\tLoss: 0.417714\n",
      "Train Epoch: 3 [320/1800 (18%)]\tLoss: 0.433637\n",
      "Train Epoch: 3 [480/1800 (26%)]\tLoss: 0.369373\n",
      "Train Epoch: 3 [640/1800 (35%)]\tLoss: 0.562820\n",
      "Train Epoch: 3 [800/1800 (44%)]\tLoss: 0.627553\n",
      "Train Epoch: 3 [960/1800 (53%)]\tLoss: 0.335507\n",
      "Train Epoch: 3 [1120/1800 (61%)]\tLoss: 0.372348\n",
      "Train Epoch: 3 [1280/1800 (70%)]\tLoss: 0.317087\n",
      "Train Epoch: 3 [1440/1800 (79%)]\tLoss: 0.308606\n",
      "Train Epoch: 3 [1600/1800 (88%)]\tLoss: 0.362842\n",
      "Train Epoch: 3 [1760/1800 (96%)]\tLoss: 0.256551\n",
      "Train Epoch: 4 [0/1800 (0%)]\tLoss: 0.332805\n",
      "Train Epoch: 4 [160/1800 (9%)]\tLoss: 0.389616\n",
      "Train Epoch: 4 [320/1800 (18%)]\tLoss: 0.478400\n",
      "Train Epoch: 4 [480/1800 (26%)]\tLoss: 0.378021\n",
      "Train Epoch: 4 [640/1800 (35%)]\tLoss: 0.330011\n",
      "Train Epoch: 4 [800/1800 (44%)]\tLoss: 0.413091\n",
      "Train Epoch: 4 [960/1800 (53%)]\tLoss: 0.234754\n",
      "Train Epoch: 4 [1120/1800 (61%)]\tLoss: 0.267281\n",
      "Train Epoch: 4 [1280/1800 (70%)]\tLoss: 0.272331\n",
      "Train Epoch: 4 [1440/1800 (79%)]\tLoss: 0.408962\n",
      "Train Epoch: 4 [1600/1800 (88%)]\tLoss: 0.236836\n",
      "Train Epoch: 4 [1760/1800 (96%)]\tLoss: 0.194082\n",
      "Train Epoch: 5 [0/1800 (0%)]\tLoss: 0.441846\n",
      "Train Epoch: 5 [160/1800 (9%)]\tLoss: 0.377684\n",
      "Train Epoch: 5 [320/1800 (18%)]\tLoss: 0.360026\n",
      "Train Epoch: 5 [480/1800 (26%)]\tLoss: 0.252223\n",
      "Train Epoch: 5 [640/1800 (35%)]\tLoss: 0.176708\n",
      "Train Epoch: 5 [800/1800 (44%)]\tLoss: 0.130324\n",
      "Train Epoch: 5 [960/1800 (53%)]\tLoss: 0.356402\n",
      "Train Epoch: 5 [1120/1800 (61%)]\tLoss: 0.222579\n",
      "Train Epoch: 5 [1280/1800 (70%)]\tLoss: 0.246688\n",
      "Train Epoch: 5 [1440/1800 (79%)]\tLoss: 0.194250\n",
      "Train Epoch: 5 [1600/1800 (88%)]\tLoss: 0.165348\n",
      "Train Epoch: 5 [1760/1800 (96%)]\tLoss: 0.109624\n",
      "Train Epoch: 6 [0/1800 (0%)]\tLoss: 0.105737\n",
      "Train Epoch: 6 [160/1800 (9%)]\tLoss: 0.182937\n",
      "Train Epoch: 6 [320/1800 (18%)]\tLoss: 0.190565\n",
      "Train Epoch: 6 [480/1800 (26%)]\tLoss: 0.260915\n",
      "Train Epoch: 6 [640/1800 (35%)]\tLoss: 0.212680\n",
      "Train Epoch: 6 [800/1800 (44%)]\tLoss: 0.294778\n",
      "Train Epoch: 6 [960/1800 (53%)]\tLoss: 0.342202\n",
      "Train Epoch: 6 [1120/1800 (61%)]\tLoss: 0.095199\n",
      "Train Epoch: 6 [1280/1800 (70%)]\tLoss: 0.109689\n",
      "Train Epoch: 6 [1440/1800 (79%)]\tLoss: 0.120422\n",
      "Train Epoch: 6 [1600/1800 (88%)]\tLoss: 0.104155\n",
      "Train Epoch: 6 [1760/1800 (96%)]\tLoss: 0.342621\n",
      "Train Epoch: 7 [0/1800 (0%)]\tLoss: 0.204438\n",
      "Train Epoch: 7 [160/1800 (9%)]\tLoss: 0.199474\n",
      "Train Epoch: 7 [320/1800 (18%)]\tLoss: 0.100323\n",
      "Train Epoch: 7 [480/1800 (26%)]\tLoss: 0.143037\n",
      "Train Epoch: 7 [640/1800 (35%)]\tLoss: 0.221463\n",
      "Train Epoch: 7 [800/1800 (44%)]\tLoss: 0.346574\n",
      "Train Epoch: 7 [960/1800 (53%)]\tLoss: 0.254508\n",
      "Train Epoch: 7 [1120/1800 (61%)]\tLoss: 0.347188\n",
      "Train Epoch: 7 [1280/1800 (70%)]\tLoss: 0.315190\n",
      "Train Epoch: 7 [1440/1800 (79%)]\tLoss: 0.103640\n",
      "Train Epoch: 7 [1600/1800 (88%)]\tLoss: 0.170563\n",
      "Train Epoch: 7 [1760/1800 (96%)]\tLoss: 0.090682\n",
      "Train Epoch: 8 [0/1800 (0%)]\tLoss: 0.173273\n",
      "Train Epoch: 8 [160/1800 (9%)]\tLoss: 0.091685\n",
      "Train Epoch: 8 [320/1800 (18%)]\tLoss: 0.230056\n",
      "Train Epoch: 8 [480/1800 (26%)]\tLoss: 0.232348\n",
      "Train Epoch: 8 [640/1800 (35%)]\tLoss: 0.160204\n",
      "Train Epoch: 8 [800/1800 (44%)]\tLoss: 0.245749\n",
      "Train Epoch: 8 [960/1800 (53%)]\tLoss: 0.163625\n",
      "Train Epoch: 8 [1120/1800 (61%)]\tLoss: 0.091078\n",
      "Train Epoch: 8 [1280/1800 (70%)]\tLoss: 0.142665\n",
      "Train Epoch: 8 [1440/1800 (79%)]\tLoss: 0.095895\n",
      "Train Epoch: 8 [1600/1800 (88%)]\tLoss: 0.281532\n",
      "Train Epoch: 8 [1760/1800 (96%)]\tLoss: 0.205174\n",
      "Train Epoch: 9 [0/1800 (0%)]\tLoss: 0.088534\n",
      "Train Epoch: 9 [160/1800 (9%)]\tLoss: 0.089325\n",
      "Train Epoch: 9 [320/1800 (18%)]\tLoss: 0.065872\n",
      "Train Epoch: 9 [480/1800 (26%)]\tLoss: 0.236706\n",
      "Train Epoch: 9 [640/1800 (35%)]\tLoss: 0.183726\n",
      "Train Epoch: 9 [800/1800 (44%)]\tLoss: 0.058322\n",
      "Train Epoch: 9 [960/1800 (53%)]\tLoss: 0.273442\n",
      "Train Epoch: 9 [1120/1800 (61%)]\tLoss: 0.073976\n",
      "Train Epoch: 9 [1280/1800 (70%)]\tLoss: 0.255315\n",
      "Train Epoch: 9 [1440/1800 (79%)]\tLoss: 0.052543\n",
      "Train Epoch: 9 [1600/1800 (88%)]\tLoss: 0.063269\n",
      "Train Epoch: 9 [1760/1800 (96%)]\tLoss: 0.136151\n",
      "Train Epoch: 10 [0/1800 (0%)]\tLoss: 0.227449\n",
      "Train Epoch: 10 [160/1800 (9%)]\tLoss: 0.059127\n",
      "Train Epoch: 10 [320/1800 (18%)]\tLoss: 0.059052\n",
      "Train Epoch: 10 [480/1800 (26%)]\tLoss: 0.068498\n",
      "Train Epoch: 10 [640/1800 (35%)]\tLoss: 0.263936\n",
      "Train Epoch: 10 [800/1800 (44%)]\tLoss: 0.110563\n",
      "Train Epoch: 10 [960/1800 (53%)]\tLoss: 0.066080\n",
      "Train Epoch: 10 [1120/1800 (61%)]\tLoss: 0.024227\n",
      "Train Epoch: 10 [1280/1800 (70%)]\tLoss: 0.096752\n",
      "Train Epoch: 10 [1440/1800 (79%)]\tLoss: 0.042530\n",
      "Train Epoch: 10 [1600/1800 (88%)]\tLoss: 0.019110\n",
      "Train Epoch: 10 [1760/1800 (96%)]\tLoss: 0.223888\n",
      "Epoch 10 finished: train loss = 0.025991292670369148, val loss = 0.4650007486343384\n",
      "elapsed_time: 83.3682177066803\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "train(10)\n",
    "elapsed_time = time.time()-start_time\n",
    "print(f'elapsed_time: {elapsed_time}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, './model.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    model.eval()\n",
    "    pred_list = []\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            data = data.to(device)\n",
    "            output = model(data)\n",
    "            pred = torch.tensor(\n",
    "                [1 if output_data >= 0.5 else 0 for output_data in output]).to(device)\n",
    "            pred_list.extend(pred)\n",
    "    return pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = torch.load(\"./model.pt\", map_location=device)\n",
    "answer = test(saved_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>image_001.png</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>image_002.png</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>image_003.png</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>image_004.png</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>image_005.png</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        filename  label\n",
       "0  image_001.png    NaN\n",
       "1  image_002.png    NaN\n",
       "2  image_003.png    NaN\n",
       "3  image_004.png    NaN\n",
       "4  image_005.png    NaN"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df = pd.read_csv('./dataset/test/submission.csv')\n",
    "submission_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>image_001.png</td>\n",
       "      <td>covid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>image_002.png</td>\n",
       "      <td>covid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>image_003.png</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>image_004.png</td>\n",
       "      <td>covid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>image_005.png</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        filename   label\n",
       "0  image_001.png   covid\n",
       "1  image_002.png   covid\n",
       "2  image_003.png  normal\n",
       "3  image_004.png   covid\n",
       "4  image_005.png  normal"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for idx, pred in enumerate(answer):\n",
    "    submission_df.loc[idx, \"label\"] = \"normal\" if (\n",
    "        int(pred) == 0) else \"covid\"\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv(\"submission_result.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
